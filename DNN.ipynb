{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import KFold, GridSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, mean_absolute_error, r2_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, OneCycleLR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import Counter\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas_folder = \"clean_datas/\"\n",
    "train_data_file = \"ecfp_train.csv\"\n",
    "test_data_file = \"ecfp_test.csv\"\n",
    "cddd_train_file = \"cddd_train.csv\"\n",
    "cddd_test_file =  \"cddd_test.csv\"\n",
    "mix_train_file = \"mix_train.csv\"\n",
    "mix_test_file =  \"mix_test.csv\"\n",
    "train = pd.read_csv(datas_folder + train_data_file)\n",
    "test = pd.read_csv(datas_folder + test_data_file)\n",
    "cddd_train = pd.read_csv(datas_folder + cddd_train_file)\n",
    "cddd_test = pd.read_csv(datas_folder + cddd_test_file)\n",
    "mix_train = pd.read_csv(datas_folder + mix_train_file)\n",
    "mix_test = pd.read_csv(datas_folder + mix_test_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    y_pred_exp = np.exp(y_pred)\n",
    "    loss = mean_squared_error(np.exp(y_true), y_pred_exp) \n",
    "    return loss\n",
    "custom_scorer = make_scorer(custom_loss, greater_is_better=False)\n",
    "\n",
    "def prep_fitt(datas):\n",
    "    datas = datas.copy()\n",
    "    try:\n",
    "        return datas.drop(labels=[\"RT\", \"SMILES\", \"mol\", \"Compound\"], axis=1)\n",
    "    except Exception:\n",
    "        return datas.drop(labels=[\"SMILES\", \"mol\", \"Compound\"], axis=1)\n",
    "    \n",
    "    \n",
    "# CDDD_datas\n",
    "#y = cddd_train[\"RT\"]\n",
    "#X = prep_fitt(cddd_train)\n",
    "#X_test = prep_fitt(cddd_test)\n",
    "\n",
    "\n",
    "# ECFP_datas\n",
    "#y = train[\"RT\"]\n",
    "#X = prep_fitt(train)\n",
    "#X_test = prep_fitt(test)\n",
    "\n",
    "\n",
    "# MIXED_datas\n",
    "#scaler = StandardScaler()\n",
    "y = mix_train[\"RT\"]\n",
    "X = prep_fitt(mix_train)\n",
    "X_test = prep_fitt(mix_test)\n",
    "# col_std = [colonne for colonne in X.columns if 'cddd' in colonne.lower()]\n",
    "# X_to_std = X[col_std]\n",
    "# scaler = StandardScaler()\n",
    "# X_to_std = scaler.fit_transform(X_to_std)\n",
    "# X[col_std] = X_to_std\n",
    "# col_std = [colonne for colonne in X_test.columns if 'cddd' in colonne.lower()]\n",
    "# X_to_std = X_test[col_std]\n",
    "# X_to_std = scaler.transform(X_to_std)\n",
    "# X_test[col_std] = X_to_std\n",
    "#X_test = pd.DataFrame(scaler.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    }
   ],
   "source": [
    "\n",
    "seed_num = 31\n",
    "torch.use_deterministic_algorithms(True)\n",
    "random.seed(seed_num)\n",
    "np.random.seed(seed_num)\n",
    "torch.manual_seed(seed_num)\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "g = torch.Generator()\n",
    "g.manual_seed(seed_num)\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FNN Model 2 Hidden-Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNNModel(nn.Module):\n",
    "    def __init__(self, input_size, n1, n2, activation, batch_normal = False ,dropout_rate = None):\n",
    "        super(DNNModel, self).__init__()\n",
    "        self.batch_normal = batch_normal\n",
    "        self.fc1 = nn.Linear(input_size, n1)\n",
    "        \n",
    "        if batch_normal : \n",
    "            self.bn1 = nn.BatchNorm1d(n1)\n",
    "            #print(\"--- std batch -----\")\n",
    "            \n",
    "        self.activation1 = nn.ReLU() if activation == 'relu' else nn.Tanh() if activation == 'tanh' else nn.Sigmoid()\n",
    "        self.fc2 = nn.Linear(n1, n2)\n",
    "        \n",
    "        if batch_normal : \n",
    "            self.bn2 = nn.BatchNorm1d(n2) \n",
    "        \n",
    "        self.activation2 = nn.ReLU() if activation == 'relu' else nn.Tanh() if activation == 'tanh' else nn.Sigmoid() \n",
    "        self.fc3 = nn.Linear(n2, 1)\n",
    "        if type(dropout_rate) == float : \n",
    "            self.dropout = nn.Dropout(dropout_rate)\n",
    "            self.drop_or_no_drop = True\n",
    "            #print(f\"\\n--- dropout: {dropout_rate} --- \\n\")\n",
    "        else : \n",
    "            self.drop_or_no_drop = False\n",
    "    \n",
    "\n",
    "    def forward(self, x):  \n",
    "        if self.batch_normal:  # BATCH NORMAL\n",
    "            if self.drop_or_no_drop:  # DROPOUT\n",
    "                x = self.activation1(self.bn1(self.fc1(x)))\n",
    "                x = self.dropout(x)\n",
    "                x = self.activation2(self.bn2(self.fc2(x)))\n",
    "                x = self.dropout(x)\n",
    "                x = self.fc3(x)\n",
    "                return x\n",
    "            else :      # NO DROPOUT\n",
    "                x = self.activation1(self.bn1(self.fc1(x)))\n",
    "                x = self.activation2(self.bn2(self.fc2(x)))\n",
    "                x = self.fc3(x)\n",
    "                return x\n",
    "        \n",
    "        else :  # NO BATCH NORMAL\n",
    "            if self.drop_or_no_drop: # DROPOUT\n",
    "                x = self.activation1(self.fc1(x))\n",
    "                x = self.dropout(x)\n",
    "                x = self.activation2(self.fc2(x))\n",
    "                x = self.dropout(x)\n",
    "                x = self.fc3(x)\n",
    "                return x\n",
    "            else :          # NO DROPOUT\n",
    "                x = self.activation1(self.fc1(x))\n",
    "                x = self.activation2(self.fc2(x))\n",
    "                x = self.fc3(x)\n",
    "                return x    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / Test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, criterion, scheduler = None): \n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #train_loss += np.sqrt(loss.item())\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        if scheduler != None :\n",
    "            scheduler.step()\n",
    "    return train_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def test(model, test_loader, criterion) : \n",
    "    model.eval()   \n",
    "    test_loss = 0.0\n",
    "    rmse_test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        inputs, labels = test_loader[:]\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss = loss.item()\n",
    "        \n",
    "        # for inputs, labels in test_loader:\n",
    "            \n",
    "        #     outputs = model(inputs)\n",
    "        #     loss = criterion(outputs, labels)\n",
    "        #     rmse_test_loss += np.sqrt(loss.item())\n",
    "        #     test_loss += loss.item()\n",
    "    \n",
    "    return [test_loss, np.sqrt(test_loss)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Validation with monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_validation(X, y, params, learning_rate = 0.001, test_size = 0.5, lr_modifier = False, batch_normal = False, verbose = False, nested_cross_validation = False, y_val = None, X_val = None) : \n",
    "    n1, n2, activation = params[\"n1\"], params[\"n2\"], params[\"activation\"]\n",
    "    try : \n",
    "        num_epochs =  params[\"num_epochs\"]\n",
    "    except : \n",
    "        num_epochs=450\n",
    "    try : \n",
    "        batch_size = params[\"batch_size\"]\n",
    "    except: \n",
    "        batch_size= None\n",
    "    try : \n",
    "        dropout_rate = params[\"dropout_rate\"]\n",
    "    except : \n",
    "        dropout_rate = None\n",
    "    try : \n",
    "        lr_max = params[\"lr_max\"]\n",
    "    except : \n",
    "        lr_max = 0.003\n",
    "    try : \n",
    "        opti = params[\"opti\"]\n",
    "        #print(activation)\n",
    "    except : \n",
    "        opti = \"adam\"\n",
    "        \n",
    "    \n",
    "    if not nested_cross_validation : \n",
    "        X = torch.tensor(X.to_numpy(), dtype=torch.float32)\n",
    "        y = torch.tensor(y.to_numpy(), dtype=torch.float32).view(-1, 1)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42, shuffle = True)  \n",
    "    else : \n",
    "        y_test = y_val\n",
    "        X_test = X_val\n",
    "        X_train = X\n",
    "        y_train = y\n",
    "        \n",
    "    train_loader = TensorDataset(X_train, y_train)\n",
    "    test_loader = TensorDataset(X_test,y_test)\n",
    "    \n",
    "    if batch_size == None : \n",
    "        #print(\"Attention no batch \\n\")\n",
    "        train_loader = DataLoader(train_loader, batch_size=len(X_train), shuffle=True, worker_init_fn= seed_worker, generator=g)\n",
    "    else : \n",
    "        #print(f\"- batch size: {batch_size}\\n\")\n",
    "        train_loader = DataLoader(train_loader, batch_size=batch_size, shuffle=True, worker_init_fn= seed_worker, generator=g)\n",
    "    \n",
    "        \n",
    "    model = DNNModel(input_size= X_train.shape[1],  n1=n1,  n2=n2, activation=activation, dropout_rate=dropout_rate, batch_normal=batch_normal)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    if opti == \"adam\" : \n",
    "        optimizer = optim.Adam(model.parameters(), lr= learning_rate) \n",
    "    \n",
    "    else : \n",
    "        optimizer = AdamW(model.parameters(), lr=0.001, weight_decay= 0.01)\n",
    "    \n",
    "    \n",
    "    if lr_modifier :\n",
    "        scheduler = OneCycleLR(optimizer, max_lr= lr_max, total_steps=num_epochs * len(train_loader))\n",
    "\n",
    "    prev_lr = None\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    rmse_loss = []\n",
    "    epochs_list = list(range(1, num_epochs + 1))\n",
    "    lr_list = []\n",
    "    \n",
    "    \n",
    "\n",
    "    for epoch in tqdm(range(num_epochs),disable = verbose,desc = \"Trainig Epochs\",unit =\"epoch\" ,colour = \"green\") : \n",
    "    \n",
    "        if lr_modifier :                \n",
    "            train_loss = train(model, train_loader, criterion= criterion, optimizer=optimizer, scheduler= scheduler)\n",
    "        else : \n",
    "            train_loss = train(model, train_loader, criterion= criterion, optimizer=optimizer)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        test_loss = test(model, test_loader, criterion=criterion)\n",
    "        test_losses.append(test_loss[0])\n",
    "        rmse_loss.append(test_loss[1])\n",
    "        \n",
    "        if lr_modifier :\n",
    "            current_lr = scheduler.optimizer.param_groups[0]['lr']\n",
    "            lr_list.append(current_lr)\n",
    "            \n",
    "            if current_lr != prev_lr:  \n",
    "                prev_lr = current_lr\n",
    "            if verbose and not nested_cross_validation: \n",
    "                print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss[0]:.4f}, Learning Rate: {current_lr} ')\n",
    "        else : \n",
    "            if verbose and not nested_cross_validation :\n",
    "                print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss[0]:.4f}\")\n",
    "        \n",
    "    model.eval()   \n",
    "    with torch.no_grad():\n",
    "            prediction = model(X_test)\n",
    "            training = model(X_train)\n",
    "        \n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test, prediction))\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_train,training))\n",
    "    if nested_cross_validation : \n",
    "        return [rmse_train, rmse_test] \n",
    "    \n",
    "    else:\n",
    "        print(f\"Final  test error : {rmse_test} | Final train error : {rmse_train} | Min test error : {np.min(rmse_loss):.4f}\")  \n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        i = test_losses.index(min(test_losses))\n",
    "        print(f\"Epoch for minimum test loss : {epochs_list[i]}\")\n",
    "        min_test_loss_value = test_losses[i]\n",
    "        min_test_loss_epoch = epochs_list[i]\n",
    "        ax.axhline(min_test_loss_value, linestyle='--', color='white', label=f'Min Test Loss: {min_test_loss_value:.4f} | epoch: {min_test_loss_epoch}')\n",
    "        ax.plot(epochs_list, train_losses, label='Train RMSE', c='#00C3FF')\n",
    "        ax.plot(epochs_list,test_losses, color=\"#FF9200\", label='Test RMSE')\n",
    "        \n",
    "        if lr_modifier : \n",
    "            ax2 = ax.twinx()\n",
    "            ax2.set_ylabel('Learning Rate', color=\"white\")\n",
    "            ax2.plot(epochs_list, lr_list, color=\"#90FF15\", label=\"Learning Rate\")\n",
    "            ax2.tick_params(axis='y', colors=\"white\")\n",
    "            ax2.legend(loc='upper center', facecolor='#2F2F2F', edgecolor='white', labelcolor='white')\n",
    "            ax2.spines['bottom'].set_color('white')  \n",
    "            ax2.spines['top'].set_color('white')    \n",
    "            ax2.spines['right'].set_color('white')   \n",
    "            ax2.spines['left'].set_color('white')    \n",
    "            ax2.xaxis.label.set_color('white')\n",
    "            ax2.yaxis.label.set_color('white')\n",
    "            \n",
    "        #ax.grid(color='#E27429', linestyle='--', linewidth=0.5)\n",
    "        ax.set_facecolor('#2F2F2F') \n",
    "        ax.spines['bottom'].set_color('white')  \n",
    "        ax.spines['top'].set_color('white')    \n",
    "        ax.spines['right'].set_color('white')   \n",
    "        ax.spines['left'].set_color('white')    \n",
    "        ax.xaxis.label.set_color('white')\n",
    "        ax.yaxis.label.set_color('white')\n",
    "        ax.tick_params(axis='x', colors='white')\n",
    "        ax.tick_params(axis='y', colors='white')\n",
    "        fig.patch.set_facecolor('#2F2F2F')\n",
    "        ax.legend(loc='upper right', facecolor='#2F2F2F', edgecolor='white', labelcolor='white')\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Loss (MSE)')\n",
    "        #ax.set_ylim(0, 2.5)\n",
    "        #ax.set_ylim(0, np.max(test_losses))\n",
    "        ax.set_ylim(0,5)\n",
    "        \n",
    "        plt.figtext(0.6, 0.63,  \n",
    "                f\" Final train loss: {train_losses[-1]:.4f} | RMSE = {rmse_train:.4f} \\n Final test loss: {test_losses[-1]:.4f} | RMSE =  {(rmse_test):.4f} \\n Min test RMSE: {np.min(rmse_loss):.4f}\", \n",
    "                horizontalalignment =\"left\", \n",
    "                wrap = True, fontsize = 10,  \n",
    "                bbox ={'facecolor':'grey',  \n",
    "                    'alpha':0.25, 'pad':5},\n",
    "                color = \"white\")\n",
    "                \n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        ax.scatter(y_test, prediction, c=\"#235a8c\", label='predictions')\n",
    "        ax.plot(y, y, color=\"#FF9200\", label='objective')\n",
    "        #ax.grid(color='white', linestyle='--', linewidth=0.5)\n",
    "        ax.errorbar(x=(min(y) + max(y)) / 2, y=(min(y) + max(y)) / 2,\n",
    "                yerr=rmse_test/2, color='#c9312c', fmt='|', markersize=5, capsize=3, label='RMSE')\n",
    "        \n",
    "        \n",
    "        ax.set_facecolor('#2F2F2F') \n",
    "        ax.spines['bottom'].set_color('white')  \n",
    "        ax.spines['top'].set_color('white')    \n",
    "        ax.spines['right'].set_color('white')   \n",
    "        ax.spines['left'].set_color('white')    \n",
    "        ax.xaxis.label.set_color('white')\n",
    "        ax.yaxis.label.set_color('white')\n",
    "        ax.tick_params(axis='x', colors='white')\n",
    "        ax.tick_params(axis='y', colors='white')\n",
    "        fig.patch.set_facecolor('#2F2F2F')\n",
    "        ax.legend(loc='upper left', facecolor='#2F2F2F', edgecolor='white', labelcolor='white')\n",
    "        ax.set_xlabel('True Values')\n",
    "        ax.set_ylabel('Predictions')\n",
    "        ax.set_title(f'RMSE = {rmse_test}', color='white')\n",
    "        plt.show()\n",
    "        \n",
    "    \n",
    "        return 0\n",
    "    \n",
    "param = {'n1': 1024, 'n2': 2048, 'activation': 'tanh', \"num_epochs\" :350, \"batch_size\" : 64, \"dropout_rate\" : 0.3, \"opti\" : \"adamW\", \"lr_max\" : 0.003}  \n",
    "simple_validation(X,y,param, test_size= 0.1, learning_rate = 0.001, lr_modifier= True, batch_normal= False, verbose=False,)  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sumbmission function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def submission(X,y,X_test,params,file_name,scheduler = False, learning_rate = 0.001) : \n",
    "    \n",
    "    n1, n2, activation = params[\"n1\"], params[\"n2\"], params[\"activation\"]\n",
    "    try : \n",
    "        num_epochs = params[\"num_epochs\"]\n",
    "    except : \n",
    "        num_epochs=450\n",
    "    try : \n",
    "        batch_size = params[\"batch_size\"]\n",
    "    except: \n",
    "        batch_size= None\n",
    "    try : \n",
    "        dropout_rate = params[\"dropout_rate\"]\n",
    "    except : \n",
    "        dropout_rate = None\n",
    "    \n",
    "    X = torch.tensor(X.to_numpy(), dtype=torch.float32)\n",
    "    y = torch.tensor(y.to_numpy(), dtype=torch.float32).view(-1, 1)\n",
    "    X_test = torch.tensor(X_test.to_numpy(), dtype=torch.float32)\n",
    "   \n",
    "    train_loader = TensorDataset(X, y)\n",
    "    train_loader = DataLoader(train_loader, batch_size= batch_size, shuffle=True, worker_init_fn= seed_worker, generator=g)\n",
    "     \n",
    "    model = DNNModel(X.shape[1], n1, n2, activation, dropout_rate= dropout_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay= 0.01)\n",
    "    #optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    if scheduler :\n",
    "        scheduler = OneCycleLR(optimizer, max_lr=0.003, total_steps= num_epochs * len(train_loader))\n",
    "    \n",
    "    else :\n",
    "        scheduler = None\n",
    "        \n",
    "    for epoch in tqdm(range(num_epochs), desc= \"Training Epochs\", unit = \"epoch\", colour = \"green\") : \n",
    "        train(model=model , train_loader=train_loader , optimizer=optimizer , criterion=criterion , scheduler=scheduler)\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prediction = model(X_test)\n",
    "        \n",
    "    sub_dataframe = pd.DataFrame(list(enumerate(prediction.numpy(), start=1)), columns=['ID', 'RT']).astype(float)\n",
    "    submission_name = f'prediction/{file_name}.csv'     \n",
    "    sub_dataframe.to_csv(submission_name, index=False)\n",
    "    torch.save(model.state_dict(), f'model/{file_name}.pth')\n",
    "    print(\"_________________\\n| Prediction Saved :D \")\n",
    "    \n",
    "\n",
    "param = {'n1': 1024, 'n2': 2048, 'activation': 'tanh', \"num_epochs\" :350, \"batch_size\" : 64, \"dropout_rate\" : 0.3, \"opti\" : \"adamW\", \"lr_max\" : 0.003} \n",
    "submission(X=X , y=y , X_test=X_test , params=param , file_name=\"DNN_mix\",scheduler= True,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_cross_validation(X,y,params, num_outer_folds=3, num_inner_folds=3, learning_rate=0.001, lr_modifier = False):\n",
    "    print(f\"NESTED CV : {num_outer_folds} outer folds | {num_inner_folds} inner fold \\n____________________________________________________________\")\n",
    "    \n",
    "    X  = torch.tensor(X.to_numpy(), dtype=torch.float32) \n",
    "    y = torch.tensor(y.to_numpy(), dtype=torch.float32).view(-1, 1)\n",
    "    outer_kf = KFold(n_splits=num_outer_folds, shuffle=True, random_state=seed_num)\n",
    "    \n",
    "    i = 1\n",
    "    all_best_param = []\n",
    "    test_score = []\n",
    "    train_score = []\n",
    "    all_test_indices = []\n",
    "    all_train_indices = []\n",
    "    #for train_index, test_index in outer_kf.split(X):\n",
    "    for train_index, test_index in tqdm(outer_kf.split(X), desc=\"Outer K-Fold\", total=outer_kf.get_n_splits(X), colour = \"green\"):\n",
    "        all_test_indices.extend(test_index)\n",
    "        all_train_indices.extend(train_index)\n",
    "        print(f\"OUTER FOLD:{i})\")\n",
    "            \n",
    "        X_train_outer, X_test_outer = X[train_index], X[test_index]\n",
    "        y_train_outer, y_test_outer = y[train_index], y[test_index]\n",
    "        outer_test = []\n",
    "        outer_train = []\n",
    "        inner_kf = KFold(n_splits=num_inner_folds, shuffle=True, random_state=seed_num)\n",
    "        t = 0 \n",
    "        for param_combination in params:\n",
    "            \n",
    "            inner_test = []\n",
    "            inner_train = []\n",
    "            for inner_train_index, inner_val_index in inner_kf.split(X_train_outer):\n",
    "                X_train_inner, X_val_inner = X_train_outer[inner_train_index], X_train_outer[inner_val_index]\n",
    "                y_train_inner, y_val_inner = y_train_outer[inner_train_index], y_train_outer[inner_val_index]\n",
    "\n",
    "                \n",
    "                accuracy = simple_validation(X=X_train_inner,y= y_train_inner,X_val= X_val_inner, y_val=y_val_inner,\n",
    "                                                params=param_combination, learning_rate=learning_rate, nested_cross_validation= True,verbose=True, lr_modifier= lr_modifier)\n",
    "                inner_train.append(accuracy[0])\n",
    "                inner_test.append(accuracy[1])\n",
    "\n",
    "            mean_inner_test = np.mean(inner_test)\n",
    "            mean_inner_train = np.mean(inner_train)\n",
    "            \n",
    "            outer_test.append(mean_inner_test)\n",
    "            outer_train.append(mean_inner_train)\n",
    "            \n",
    "            print(f\"    Param {t}) mean RMSE valid: {mean_inner_test} | train:{mean_inner_train}\")\n",
    "            t+=1\n",
    "        best_param_combination = params[np.argmin(outer_test)]\n",
    "        all_best_param.append(best_param_combination)\n",
    "        print(f\"    Best param : {all_best_param.index(best_param_combination) +1}\")\n",
    "        \n",
    "        \n",
    "        train_test_accuracy = simple_validation(X=X_train_outer,y= y_train_outer,X_val= X_test_outer, y_val=y_test_outer,\n",
    "                                                params=best_param_combination, learning_rate=learning_rate, nested_cross_validation= True,verbose=True, lr_modifier= lr_modifier)\n",
    "        train_score.append(train_test_accuracy[0])\n",
    "        test_score.append(train_test_accuracy[1])\n",
    "        i += 1\n",
    "        print(f\"RMSE test: {train_test_accuracy[1]} | train: {train_test_accuracy[0]} \\n____________________________________________________________\")\n",
    "    \n",
    "    \n",
    "    counters = {key: Counter(param[key] for param in all_best_param) for key in all_best_param[0]}\n",
    "    most_frequent_values = {key: counter.most_common(1)[0][0] for key, counter in counters.items()}\n",
    "    print(f\"|BEST PARAM:{most_frequent_values}\\n\")\n",
    "    return most_frequent_values\n",
    "\n",
    "\n",
    "param = [                                   \n",
    "    {'n1': 1024, 'n2': 2048, 'activation': 'tanh', \"num_epochs\" : 300, \"batch_size\" : 64, \"dropout_rate\" : 0.3, \"lr_max\" : 0.0035},\n",
    "    {'n1': 1024, 'n2': 2048, 'activation': 'tanh', \"num_epochs\" : 300, \"batch_size\" : 64, \"dropout_rate\" : 0.3},\n",
    "    {'n1': 1024, 'n2': 1024, 'activation': 'tanh', \"num_epochs\" : 300, \"batch_size\" : 64, \"dropout_rate\" : 0.3},\n",
    "    {'n1': 1024, 'n2': 2048, 'activation': 'tanh', \"num_epochs\" : 300, \"batch_size\" : 64, \"dropout_rate\" :0.3, \"opti\" : \"adamW\"},\n",
    "    {'n1': 1024, 'n2': 2048, 'activation': 'tanh', \"num_epochs\" : 300, \"batch_size\" : 64, \"dropout_rate\" : 0.3, \"lr_max\" : 0.0025 },\n",
    "    ]\n",
    "\n",
    "\n",
    "nested_cross_validation(X,y,param,3,3, lr_modifier=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing test CDDD datas resolving for Kaggle submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________\n",
      "|Complete Done :D\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def missing_cddd(file_to_change, final_file_name = None) : \n",
    "    if final_file_name == None: \n",
    "        final_file_name == file_to_change\n",
    "    datas_folder = \"prediction/\"\n",
    "    missing_test_cddd = [279, 462, 695, 883, 913, 1202]\n",
    "    sub_cddd = pd.read_csv(f\"prediction/{file_to_change}.csv\")\n",
    "    sub_ecfp= pd.read_csv(\"prediction\\DNN_ECFP.csv\")\n",
    "    sub_cddd.iloc[missing_test_cddd] = sub_ecfp.iloc[missing_test_cddd].values\n",
    "    sub_cddd.to_csv(f\"prediction/{final_file_name}.csv\".format(datas_folder), index=False)\n",
    "    print(\"_____________________\\n|Complete Done :D\")\n",
    "    return 0 \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
